{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4435e4d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading langchain-0.1.14-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.9.3-cp39-cp39-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.30 (from langchain)\n",
      "  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.37 (from langchain)\n",
      "  Downloading langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.40-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.0.5-cp39-cp39-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.9.4-cp39-cp39-win_amd64.whl.metadata (32 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.37->langchain)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.0-cp39-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.7/50.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Collecting typing-extensions>=4.2.0 (from pydantic<3,>=1->langchain)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Downloading langchain-0.1.14-py3-none-any.whl (812 kB)\n",
      "   ---------------------------------------- 0.0/812.8 kB ? eta -:--:--\n",
      "   --------------------------------------  809.0/812.8 kB 25.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 812.8/812.8 kB 17.1 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.3-cp39-cp39-win_amd64.whl (366 kB)\n",
      "   ---------------------------------------- 0.0/366.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 366.0/366.0 kB ? eta 0:00:00\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.9/1.9 MB 60.2 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.1.40-py3-none-any.whl (276 kB)\n",
      "   ---------------------------------------- 0.0/276.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 276.8/276.8 kB 17.8 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Downloading langsmith-0.1.40-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.5/87.5 kB ? eta 0:00:00\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp39-cp39-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.7/50.7 kB ? eta 0:00:00\n",
      "Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.4/49.4 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Downloading orjson-3.10.0-cp39-none-win_amd64.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.1/139.1 kB ? eta 0:00:00\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.9.4-cp39-cp39-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.9/76.9 kB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: typing-extensions, packaging, orjson, multidict, jsonpatch, frozenlist, async-timeout, yarl, typing-inspect, marshmallow, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 dataclasses-json-0.6.4 frozenlist-1.4.1 jsonpatch-1.33 langchain-0.1.14 langchain-community-0.0.31 langchain-core-0.1.40 langchain-text-splitters-0.0.1 langsmith-0.1.40 marshmallow-3.21.1 multidict-6.0.5 orjson-3.10.0 packaging-23.2 typing-extensions-4.10.0 typing-inspect-0.9.0 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\deepak\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepak\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\deepak\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepak\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-models-official 2.16.0 requires google-api-python-client>=1.6.7, which is not installed.\n",
      "tf-models-official 2.16.0 requires immutabledict, which is not installed.\n",
      "tf-models-official 2.16.0 requires kaggle>=1.3.9, which is not installed.\n",
      "tf-models-official 2.16.0 requires oauth2client, which is not installed.\n",
      "tf-models-official 2.16.0 requires sentencepiece, which is not installed.\n",
      "tf-models-official 2.16.0 requires seqeval, which is not installed.\n",
      "tf-models-official 2.16.0 requires tensorflow-hub>=0.6.0, which is not installed.\n",
      "tf-models-official 2.16.0 requires tensorflow-model-optimization>=0.4.1, which is not installed.\n",
      "tf-models-official 2.16.0 requires tensorflow-text~=2.16.1, which is not installed.\n",
      "apache-beam 2.55.0 requires crcmod<2.0,>=1.7, which is not installed.\n",
      "apache-beam 2.55.0 requires fastavro<2,>=0.23.6, which is not installed.\n",
      "apache-beam 2.55.0 requires fasteners<1.0,>=0.3, which is not installed.\n",
      "apache-beam 2.55.0 requires hdfs<3.0.0,>=2.1.0, which is not installed.\n",
      "apache-beam 2.55.0 requires httplib2<0.23.0,>=0.8, which is not installed.\n",
      "apache-beam 2.55.0 requires js2py<1,>=0.74, which is not installed.\n",
      "apache-beam 2.55.0 requires jsonpickle<4.0.0,>=3.0.0, which is not installed.\n",
      "apache-beam 2.55.0 requires objsize<0.8.0,>=0.6.1, which is not installed.\n",
      "apache-beam 2.55.0 requires proto-plus<2,>=1.7.1, which is not installed.\n",
      "apache-beam 2.55.0 requires pyarrow-hotfix<1, which is not installed.\n",
      "apache-beam 2.55.0 requires pydot<2,>=1.2.0, which is not installed.\n",
      "apache-beam 2.55.0 requires pymongo<5.0.0,>=3.8.0, which is not installed.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.10.0 which is incompatible.\n",
      "object-detection 0.1 requires pyparsing==2.4.7, but you have pyparsing 3.0.9 which is incompatible.\n",
      "tf-models-official 2.16.0 requires tensorflow~=2.16.1, but you have tensorflow 2.13.0 which is incompatible.\n",
      "apache-beam 2.55.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n",
      "apache-beam 2.55.0 requires pyarrow<12.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b88014b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "220ab189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eeca4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a web scraping specialist.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab44ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bf12364",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ad6df",
   "metadata": {},
   "source": [
    "# Retrieval Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5b93936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://www.tutorialspoint.com/beautiful_soup/beautiful_soup_quick_guide.htm\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "103ac837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ba4b40a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0-cp39-cp39-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.24.3)\n",
      "Downloading faiss_cpu-1.8.0-cp39-cp39-win_amd64.whl (14.5 MB)\n",
      "   ---------------------------------------- 0.0/14.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.5 MB 13.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.5/14.5 MB 32.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 5.4/14.5 MB 43.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.9/14.5 MB 45.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.1/14.5 MB 46.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 11.6/14.5 MB 50.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.3/14.5 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.9/14.5 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.5/14.5 MB 38.6 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\deepak\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepak\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\deepak\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\deepak\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50795d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fdcb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ab5b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a451d644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautiful Soup is a Python library used for parsing and scraping HTML and XML documents. It provides a simple and easy-to-use way to navigate and search through the contents of web pages, documents, and other structured content. Here are some common use cases for Beautiful Soup:\n",
      "\n",
      "1. Web scraping: Beautiful Soup can be used to extract data from websites, such as prices, reviews, or news articles, by navigating through the HTML structure of the page.\n",
      "2. Data mining: The library can be used to extract large amounts of data from websites, which can then be processed and analyzed using Python's data science libraries.\n",
      "3. Web automation: Beautiful Soup can be used to automate tasks on a website, such as filling out forms or clicking buttons, by simulating user interaction with the HTML structure of the page.\n",
      "4. Reverse engineering: The library can be used to analyze the structure and content of websites, including the use of hidden elements or JavaScript-generated content, which may not be visible to users.\n",
      "5. Data visualization: Beautiful Soup can be used to extract data from websites and pass it to other Python libraries for visualization, such as Matplotlib or Plotly.\n",
      "6. Web development: Beautiful Soup can be used to create web scrapers or web crawlers, which can be integrated into web applications to fetch data from external sources or monitor changes on the web.\n",
      "7. Research and development: The library can be used for research purposes, such as analyzing the structure of websites, identifying patterns in content, or testing the accessibility of websites.\n",
      "8. Education: Beautiful Soup can be used in academic settings to teach students about web development, data science, and programming concepts.\n",
      "\n",
      "Overall, Beautiful Soup is a versatile library that can be used for a wide range of tasks involving structured content on the web.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"What is Beautiful Soup used for?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "196dc18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To parse a webpage using Beautiful Soup, you can use the following code as a starting point:\n",
      "```python\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Load the webpage into a string\n",
      "url = \"https://www.example.com\"\n",
      "page = requests.get(url).text\n",
      "\n",
      "# Create a Beautiful Soup object\n",
      "soup = BeautifulSoup(page, \"html.parser\")\n",
      "\n",
      "# Find the element you want to parse\n",
      "element = soup.find(\"div\", class_=\"my-class\")\n",
      "\n",
      "# Do something with the element\n",
      "print(element.name)\n",
      "```\n",
      "In this code:\n",
      "\n",
      "* `requests` is a Python package for making HTTP requests. You can replace `requests.get()` with any other way you have of loading the webpage, such as using `requests.get()` with a URL parameter.\n",
      "* The string `page` contains the HTML content of the webpage.\n",
      "* The `BeautifulSoup()` constructor takes the HTML content as input and returns a `BeautifulSoup` object.\n",
      "* The `find()` method searches for an element on the page that matches the given selector (in this case, a `<div>` element with a class of `\"my-class\"`). If the method finds the element, it returns a `Tag` object representing the element.\n",
      "* The `name` attribute of the element is printed to the console.\n",
      "\n",
      "Of course, this is just a basic example â€“ you can use Beautiful Soup to parse any webpage, and do whatever you want with the elements on the page. For example, you might want to extract information from the webpage, manipulate the elements on the page, or even completely rewrite the page's content. The possibilities are endless!\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"What is the code to parse a webpage?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
